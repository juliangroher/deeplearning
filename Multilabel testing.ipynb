{"cells":[{"cell_type":"code","source":"import pandas as pd \n\ndef filter_labels(dataframe, label_filter=None):\n    # label_filter can be used to only consider images\n    # with a certain label\n    # Can be used to build a primitve classifier\n    # considering only one class of labels\n    for idx, row in dataframe.iterrows():\n        tags = row['tags']\n        labels = tags.strip().split(' ')\n        intersection = set(label_filter).intersection(set(labels))\n        intersectionList = list(intersection)\n\n        # only considers one (first) tag atm\n        dataframe.loc[idx].tags = intersectionList[0] if intersectionList else None\n\n    dataframe = dataframe.dropna()\n    return dataframe","metadata":{"tags":[],"cell_id":"d2e0388aa6074c6b81086e5583c78380","source_hash":"d53fc2ef","execution_start":1669369920876,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!ls /datasets/deeplearningproject","metadata":{"tags":[],"cell_id":"657ccead3f8f4a14b4ede0b6527f7390","source_hash":"15eca41e","execution_start":1669369920877,"execution_millis":770,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"'Colab Notebooks'    train-jpg\t        train-jpg-small\n train_classes.csv   train-jpg-almost   train-jpg.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"local = False","metadata":{"tags":[],"cell_id":"a5611eb171a04fefa796d84ec23f942e","source_hash":"c7512eea","execution_start":1669369921652,"execution_millis":43,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"label_path = 'datasets/deeplearningproject/train_classes.csv'\nif not local:\n    label_path = '/' + label_path\nlabel_df = pd.read_csv(label_path)\n\ncloudy_labels = ['clear', 'partly_cloudy', 'cloudy', 'haze']\n\n# We need to remember to exclude the images which have been filtered here\nlabel_mapping = filter_labels(label_df, label_filter=cloudy_labels)","metadata":{"tags":[],"cell_id":"35ab3fffc9d740fcb0c4f9a65fba0c5d","source_hash":"fe4406a1","execution_start":1669369921695,"execution_millis":6818,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nimport torch\nimport torchvision.transforms as transforms\n\n# Dataset subclass for our own image set\n# TODO: change to support multiple labels\nclass CustomImageDataset(Dataset):\n    def __init__(self, img_labels, img_dir, transform=None, target_transform=None, preprocessor=None):\n        self.img_labels = img_labels#pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.preprocessor = preprocessor\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] + '.jpg')\n        image = read_image(img_path)\n        image = image [:3,:,:] #remove alpha channel\n          \n        if self.preprocessor != None:\n            image = image/255 #required for preprocessor | normalize each picture\n            image = self.preprocessor(image)\n        else:\n            image = 2 * (image/255 - 0.5) #normalize each picture\n            \n        label = self.img_labels.iloc[idx, 1]\n        label = cloudy_labels.index(label) # TODO multilabel\n\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"tags":[],"cell_id":"6330b9a4a52b4356846ebfe390c78360","source_hash":"b1a7705a","execution_start":1669369928520,"execution_millis":2087,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# First import what we need\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"tags":[],"cell_id":"bd92eee1ca804cacad177677634a0cb4","source_hash":"72f6ae7d","execution_start":1669369930610,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def reset_parameters(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            torch.nn.init.kaiming_normal_(m.weight)\n            if m.bias is not None:\n                torch.nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            torch.nn.init.constant_(m.weight, 1) # Why 1?\n            torch.nn.init.constant_(m.bias, 0) # Why 0?\n        elif isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_normal_(m.weight)\n            if m.bias is not None:\n                torch.nn.init.constant_(m.bias, 0)\n\n#reset_parameters(model)","metadata":{"tags":[],"cell_id":"5650ec94cb1f4897afd6da801a4d5b2e","source_hash":"88c39b50","execution_start":1669369930616,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nnetwork_weights = MobileNet_V3_Small_Weights.DEFAULT\n\ndef Net():\n    encoder = mobilenet_v3_small(weights=network_weights)\n    # encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n    for param in encoder.parameters():\n        param.requires_grad = False\n\n\n    decoder = nn.Sequential(\n        nn.Dropout(p=0.5),\n        nn.Linear(in_features=576,out_features=120),\n        nn.ReLU(),\n        nn.Linear(in_features=120, out_features=17),\n        nn.Sigmoid()\n    )\n    for param in decoder.parameters():\n        param.requires_grad=True\n\n    # encoder.fc = decoder\n    encoder.classifier = decoder\n    # net = nn.Sequential(\n    #     encoder,\n    #     decoder\n    # )\n    return encoder\n\nmodel = Net()\n\nprint('Network parameters:\\n')\nprint(model)\n\n# Print parameter shapes\nfor name, param in model.named_parameters(): print('parameter',name,param.shape)","metadata":{"tags":[],"cell_id":"4b7aed3c4e484fdeac033de70b9bfb62","source_hash":"110abd7e","execution_start":1669369930624,"execution_millis":295,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n100%|██████████| 9.83M/9.83M [00:00<00:00, 380MB/s]Network parameters:\n\nMobileNetV3(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n    (1): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (2): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (3): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (4): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (5): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (8): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (9): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (10): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (11): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (12): Conv2dNormActivation(\n      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=576, out_features=120, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=120, out_features=17, bias=True)\n    (4): Sigmoid()\n  )\n)\nparameter features.0.0.weight torch.Size([16, 3, 3, 3])\nparameter features.0.1.weight torch.Size([16])\nparameter features.0.1.bias torch.Size([16])\nparameter features.1.block.0.0.weight torch.Size([16, 1, 3, 3])\nparameter features.1.block.0.1.weight torch.Size([16])\nparameter features.1.block.0.1.bias torch.Size([16])\nparameter features.1.block.1.fc1.weight torch.Size([8, 16, 1, 1])\nparameter features.1.block.1.fc1.bias torch.Size([8])\nparameter features.1.block.1.fc2.weight torch.Size([16, 8, 1, 1])\nparameter features.1.block.1.fc2.bias torch.Size([16])\nparameter features.1.block.2.0.weight torch.Size([16, 16, 1, 1])\nparameter features.1.block.2.1.weight torch.Size([16])\nparameter features.1.block.2.1.bias torch.Size([16])\nparameter features.2.block.0.0.weight torch.Size([72, 16, 1, 1])\nparameter features.2.block.0.1.weight torch.Size([72])\nparameter features.2.block.0.1.bias torch.Size([72])\nparameter features.2.block.1.0.weight torch.Size([72, 1, 3, 3])\nparameter features.2.block.1.1.weight torch.Size([72])\nparameter features.2.block.1.1.bias torch.Size([72])\nparameter features.2.block.2.0.weight torch.Size([24, 72, 1, 1])\nparameter features.2.block.2.1.weight torch.Size([24])\nparameter features.2.block.2.1.bias torch.Size([24])\nparameter features.3.block.0.0.weight torch.Size([88, 24, 1, 1])\nparameter features.3.block.0.1.weight torch.Size([88])\nparameter features.3.block.0.1.bias torch.Size([88])\nparameter features.3.block.1.0.weight torch.Size([88, 1, 3, 3])\nparameter features.3.block.1.1.weight torch.Size([88])\nparameter features.3.block.1.1.bias torch.Size([88])\nparameter features.3.block.2.0.weight torch.Size([24, 88, 1, 1])\nparameter features.3.block.2.1.weight torch.Size([24])\nparameter features.3.block.2.1.bias torch.Size([24])\nparameter features.4.block.0.0.weight torch.Size([96, 24, 1, 1])\nparameter features.4.block.0.1.weight torch.Size([96])\nparameter features.4.block.0.1.bias torch.Size([96])\nparameter features.4.block.1.0.weight torch.Size([96, 1, 5, 5])\nparameter features.4.block.1.1.weight torch.Size([96])\nparameter features.4.block.1.1.bias torch.Size([96])\nparameter features.4.block.2.fc1.weight torch.Size([24, 96, 1, 1])\nparameter features.4.block.2.fc1.bias torch.Size([24])\nparameter features.4.block.2.fc2.weight torch.Size([96, 24, 1, 1])\nparameter features.4.block.2.fc2.bias torch.Size([96])\nparameter features.4.block.3.0.weight torch.Size([40, 96, 1, 1])\nparameter features.4.block.3.1.weight torch.Size([40])\nparameter features.4.block.3.1.bias torch.Size([40])\nparameter features.5.block.0.0.weight torch.Size([240, 40, 1, 1])\nparameter features.5.block.0.1.weight torch.Size([240])\nparameter features.5.block.0.1.bias torch.Size([240])\nparameter features.5.block.1.0.weight torch.Size([240, 1, 5, 5])\nparameter features.5.block.1.1.weight torch.Size([240])\nparameter features.5.block.1.1.bias torch.Size([240])\nparameter features.5.block.2.fc1.weight torch.Size([64, 240, 1, 1])\nparameter features.5.block.2.fc1.bias torch.Size([64])\nparameter features.5.block.2.fc2.weight torch.Size([240, 64, 1, 1])\nparameter features.5.block.2.fc2.bias torch.Size([240])\nparameter features.5.block.3.0.weight torch.Size([40, 240, 1, 1])\nparameter features.5.block.3.1.weight torch.Size([40])\nparameter features.5.block.3.1.bias torch.Size([40])\nparameter features.6.block.0.0.weight torch.Size([240, 40, 1, 1])\nparameter features.6.block.0.1.weight torch.Size([240])\nparameter features.6.block.0.1.bias torch.Size([240])\nparameter features.6.block.1.0.weight torch.Size([240, 1, 5, 5])\nparameter features.6.block.1.1.weight torch.Size([240])\nparameter features.6.block.1.1.bias torch.Size([240])\nparameter features.6.block.2.fc1.weight torch.Size([64, 240, 1, 1])\nparameter features.6.block.2.fc1.bias torch.Size([64])\nparameter features.6.block.2.fc2.weight torch.Size([240, 64, 1, 1])\nparameter features.6.block.2.fc2.bias torch.Size([240])\nparameter features.6.block.3.0.weight torch.Size([40, 240, 1, 1])\nparameter features.6.block.3.1.weight torch.Size([40])\nparameter features.6.block.3.1.bias torch.Size([40])\nparameter features.7.block.0.0.weight torch.Size([120, 40, 1, 1])\nparameter features.7.block.0.1.weight torch.Size([120])\nparameter features.7.block.0.1.bias torch.Size([120])\nparameter features.7.block.1.0.weight torch.Size([120, 1, 5, 5])\nparameter features.7.block.1.1.weight torch.Size([120])\nparameter features.7.block.1.1.bias torch.Size([120])\nparameter features.7.block.2.fc1.weight torch.Size([32, 120, 1, 1])\nparameter features.7.block.2.fc1.bias torch.Size([32])\nparameter features.7.block.2.fc2.weight torch.Size([120, 32, 1, 1])\nparameter features.7.block.2.fc2.bias torch.Size([120])\nparameter features.7.block.3.0.weight torch.Size([48, 120, 1, 1])\nparameter features.7.block.3.1.weight torch.Size([48])\nparameter features.7.block.3.1.bias torch.Size([48])\nparameter features.8.block.0.0.weight torch.Size([144, 48, 1, 1])\nparameter features.8.block.0.1.weight torch.Size([144])\nparameter features.8.block.0.1.bias torch.Size([144])\nparameter features.8.block.1.0.weight torch.Size([144, 1, 5, 5])\nparameter features.8.block.1.1.weight torch.Size([144])\nparameter features.8.block.1.1.bias torch.Size([144])\nparameter features.8.block.2.fc1.weight torch.Size([40, 144, 1, 1])\nparameter features.8.block.2.fc1.bias torch.Size([40])\nparameter features.8.block.2.fc2.weight torch.Size([144, 40, 1, 1])\nparameter features.8.block.2.fc2.bias torch.Size([144])\nparameter features.8.block.3.0.weight torch.Size([48, 144, 1, 1])\nparameter features.8.block.3.1.weight torch.Size([48])\nparameter features.8.block.3.1.bias torch.Size([48])\nparameter features.9.block.0.0.weight torch.Size([288, 48, 1, 1])\nparameter features.9.block.0.1.weight torch.Size([288])\nparameter features.9.block.0.1.bias torch.Size([288])\nparameter features.9.block.1.0.weight torch.Size([288, 1, 5, 5])\nparameter features.9.block.1.1.weight torch.Size([288])\nparameter features.9.block.1.1.bias torch.Size([288])\nparameter features.9.block.2.fc1.weight torch.Size([72, 288, 1, 1])\nparameter features.9.block.2.fc1.bias torch.Size([72])\nparameter features.9.block.2.fc2.weight torch.Size([288, 72, 1, 1])\nparameter features.9.block.2.fc2.bias torch.Size([288])\nparameter features.9.block.3.0.weight torch.Size([96, 288, 1, 1])\nparameter features.9.block.3.1.weight torch.Size([96])\nparameter features.9.block.3.1.bias torch.Size([96])\nparameter features.10.block.0.0.weight torch.Size([576, 96, 1, 1])\nparameter features.10.block.0.1.weight torch.Size([576])\nparameter features.10.block.0.1.bias torch.Size([576])\nparameter features.10.block.1.0.weight torch.Size([576, 1, 5, 5])\nparameter features.10.block.1.1.weight torch.Size([576])\nparameter features.10.block.1.1.bias torch.Size([576])\nparameter features.10.block.2.fc1.weight torch.Size([144, 576, 1, 1])\nparameter features.10.block.2.fc1.bias torch.Size([144])\nparameter features.10.block.2.fc2.weight torch.Size([576, 144, 1, 1])\nparameter features.10.block.2.fc2.bias torch.Size([576])\nparameter features.10.block.3.0.weight torch.Size([96, 576, 1, 1])\nparameter features.10.block.3.1.weight torch.Size([96])\nparameter features.10.block.3.1.bias torch.Size([96])\nparameter features.11.block.0.0.weight torch.Size([576, 96, 1, 1])\nparameter features.11.block.0.1.weight torch.Size([576])\nparameter features.11.block.0.1.bias torch.Size([576])\nparameter features.11.block.1.0.weight torch.Size([576, 1, 5, 5])\nparameter features.11.block.1.1.weight torch.Size([576])\nparameter features.11.block.1.1.bias torch.Size([576])\nparameter features.11.block.2.fc1.weight torch.Size([144, 576, 1, 1])\nparameter features.11.block.2.fc1.bias torch.Size([144])\nparameter features.11.block.2.fc2.weight torch.Size([576, 144, 1, 1])\nparameter features.11.block.2.fc2.bias torch.Size([576])\nparameter features.11.block.3.0.weight torch.Size([96, 576, 1, 1])\nparameter features.11.block.3.1.weight torch.Size([96])\nparameter features.11.block.3.1.bias torch.Size([96])\nparameter features.12.0.weight torch.Size([576, 96, 1, 1])\nparameter features.12.1.weight torch.Size([576])\nparameter features.12.1.bias torch.Size([576])\nparameter classifier.1.weight torch.Size([120, 576])\nparameter classifier.1.bias torch.Size([120])\nparameter classifier.3.weight torch.Size([17, 120])\nparameter classifier.3.bias torch.Size([17])\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#weights = ResNet50_Weights.DEFAULT\n#preprocess = weights.transforms()\n#\n#img = images[0]\n#print(img.shape)\n#img_transformed = preprocess(img)\n#print(img_transformed.shape)\n#\n#scores = model(images)\n#print(scores.shape)","metadata":{"tags":[],"cell_id":"11de6fe699974f1ba8fdd3a7ca04db41","source_hash":"81cc7f59","execution_start":1669369930807,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def accuracy(scores, yb):\n    # print(\"scores\", scores.dtype)\n    # print(\"yb\", yb.dtype)\n    # Binarize predictions via thresholding\n    scores[scores>=0.5] = 1\n\n    return (scores == yb).float().mean()\n\n#print('Accuracy', accuracy(scores,labels))\n\n# loss_func = F.cross_entropy\nloss_func = torch.nn.BCELoss()\n#loss = loss_func(scores, labels)\n#print('Loss', loss)","metadata":{"tags":[],"cell_id":"dd806cf511d246789cdd69ade5f973ff","source_hash":"ee94e884","execution_start":1669369930808,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"images_path = 'datasets/temp-small-subset'\n# images_path = '/datasets/deeplearningproject/train-jpg'\nif not local:\n    images_path = '/' + images_path\n\npreprocess = network_weights.transforms()\n\ntrainset = CustomImageDataset(\n    label_mapping.iloc[:400],\n    images_path,\n    preprocessor=preprocess\n)\n\ntestset = CustomImageDataset(\n    label_mapping.iloc[400:500],\n    images_path,\n    preprocessor=preprocess\n)","metadata":{"tags":[],"cell_id":"9b23efbe7d4b451193df9c590413d2f2","source_hash":"f3c14578","execution_start":1669369930808,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom time import process_time\n\nclass MultilabelDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None, target_transform=None, preprocessor=None):\n        self.img_dataframe = dataframe  # pd.read_csv(annotation_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.preprocessor = preprocessor\n\n        # Encode labels\n        tags = ['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn',\n         'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming',\n         'selective_logging', 'blow_down']\n        split_tags = [row.split(\" \") for row in self.img_dataframe[\"tags\"]]\n        mlb = MultiLabelBinarizer(classes=tags)\n        mlb.fit(split_tags)\n        self.img_labels = mlb.transform(split_tags).astype('float32')  # BCELoss does not accept integers *for some reason*\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        # print(idx)\n        # print(self.img_dataframe[idx-10:idx+10])\n        img_path = os.path.join(self.img_dir, self.img_dataframe.iloc[idx, 0] + '.jpg')\n        # print(img_path)\n        pre_time = process_time()\n        image = read_image(img_path)\n        image = image [:3,:,:] #remove alpha channel\n        loaded_time = process_time()\n\n        if self.preprocessor != None:\n            image = image/255 #required for preprocessor | normalize each picture\n            image = self.preprocessor(image)\n        else:\n            image = 2 * (image/255 - 0.5) #normalize each picture\n\n        processed_time = process_time()\n        labels = self.img_labels[idx]\n\n        transform_time = process_time()\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            labels = self.target_transform(labels)\n\n        post_time = process_time()\n\n        # print(\"load\", loaded_time - pre_time, \"processing\", processed_time - loaded_time, \"labels\", transform_time - processed_time, \"transform\", post_time - transform_time)\n        # print(\"total\", post_time - pre_time)\n        return image, labels","metadata":{"tags":[],"cell_id":"8104e336c8fb4cf5b6986a6a564b6182","source_hash":"a75c275e","execution_start":1669369930813,"execution_millis":546,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# images_path = 'datasets/temp-small-subset'\nimages_path = '/datasets/deeplearningproject/train-jpg'\nif not local:\n    images_path = '/' + images_path\n\npreprocess = network_weights.transforms()\n\nlabel_path = \"/datasets/deeplearningproject/train_classes.csv\"\nlabels = pd.read_csv(label_path)\n\ntrain_data, test_daa = train_test_split(labels[:500], train_size=0.9)\n\ntrainset = MultilabelDataset(\n    train_data,\n    images_path,\n    preprocessor=preprocess\n)\n\ntestset = MultilabelDataset(\n    test_daa,\n    images_path,\n    preprocessor=preprocess\n)","metadata":{"tags":[],"cell_id":"d75acf46db9a4f4ea2236e4861042197","source_hash":"55d7171b","execution_start":1669369931363,"execution_millis":88,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torch import optim\ndef adam_optimizer(model):\n  return optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)","metadata":{"tags":[],"cell_id":"80ad2cb66204426d81710019a1c9ca3d","source_hash":"f3cd4e03","execution_start":1669369931495,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch import optim\nimport matplotlib.pyplot as plt\n\n# Function handle that returns an optimizer\ndef base_optimizer(model,lr=0.001, momentum=0.9):\n    return optim.SGD(model.parameters(), lr=lr,momentum=momentum)\n\n# Function handle that updates the learning rate\n# (note this is a dummy implementation that does nothing)\ndef base_lr_scheduler(t,T,lr):\n  return lr\n\n# Function to fit a model\ndef fit(model,\n        opt_func=adam_optimizer,\n        lr_scheduler=base_lr_scheduler,\n        bs=256,\n        epochs=1,\n        batches_per_epoch=None, # Default: Use entire training set\n        show_summary=True,\n        workers=0):\n\n  # Set up data loaders\n  if batches_per_epoch == None:\n    # Use all images\n    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs,\n                                          shuffle=True, num_workers=workers, persistent_workers=workers)\n    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs,\n                                         shuffle=False, num_workers=workers, persistent_workers=workers)\n    batches_per_epoch = len(train_dl)\n  else:\n    # Only use a subset of the data\n    subset_indices = list(range(batches_per_epoch*bs))\n    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=workers, persistent_workers=workers)\n\n    # Use one fourth for validation\n    subset_indices = list(range(int(np.ceil(batches_per_epoch/4))*bs))\n    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=workers, persistent_workers=workers)\n\n  # Initialize optimizer\n  opt = opt_func(model)\n\n  # For book keeping\n  train_loss_history = []\n  valid_loss_history = []\n  plot_time_train = []\n  plot_time_valid = []\n\n  # Index of current batch\n  t = 1\n\n  # Total number of batches\n  T = batches_per_epoch * epochs\n  \n  print('Epochs:',epochs,'Batches per epoch:',batches_per_epoch,'Total number of batches',T)\n  \n  # Get initial validation loss and accuracy\n  model.eval()\n  with torch.no_grad():\n    if torch.cuda.is_available():\n        valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n        valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n        valid_loss_history.append(valid_loss.detach().cpu().numpy())\n        plot_time_valid.append(t)\n    else:\n        valid_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n        valid_loss_history.append(valid_loss.detach().cpu().numpy())\n        plot_time_valid.append(t)\n\n  # Train\n  for epoch in range(epochs):\n    model.train() # Train mode\n    for xb, yb in train_dl:\n      # Update learning rate\n      opt.param_groups[0]['lr'] = lr_scheduler(t,T,lr=opt.param_groups[0]['lr'])\n\n      # Forward prop\n      \n      if torch.cuda.is_available():\n        pred = model(xb.cuda())\n        loss = loss_func(pred, yb.cuda())\n      else:\n        pred = model(xb)\n        # print(pred.shape)\n        # print(yb.shape)\n        # print(pred)\n        # print(yb)\n        loss = loss_func(pred, yb) \n\n      # Book keeping\n      train_loss_history.append(loss.detach().cpu().numpy())\n      plot_time_train.append(t)\n      t += 1\n\n      # Backward prop (calculate gradient)\n      \n      loss.backward()\n\n      # Update model parameters\n      \n      opt.step()\n      opt.zero_grad()\n      \n      # Validation loss and accuracy\n      if t % 10 == 0:    # print every 10 mini-batches\n        model.eval() # Test mode\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n                valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n                valid_loss_history.append(valid_loss.detach().cpu().numpy())\n                plot_time_valid.append(t-1)\n                print('t',t,'lr',opt.param_groups[0]['lr'],'train loss',loss.detach().cpu().numpy(), 'val loss',valid_loss.detach().cpu().numpy(),'val accuracy', valid_acc.detach().cpu().numpy())\n            else:\n                valid_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n                valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n                valid_loss_history.append(valid_loss.detach().cpu().numpy())\n                plot_time_valid.append(t-1)\n                print('t',t,'lr',opt.param_groups[0]['lr'],'train loss',loss.detach().cpu().numpy(), 'val loss',valid_loss.detach().cpu().numpy(),'val accuracy', valid_acc.detach().cpu().numpy())\n        model.train() # Back to train mode\n\n  # Summary\n  if show_summary:\n    plt.figure()\n    lines = []\n    labels = []\n    l, = plt.plot(plot_time_train,train_loss_history)\n    lines.append(l)\n    labels.append('Training')\n    l, = plt.plot(plot_time_valid,valid_loss_history)\n    lines.append(l)\n    labels.append('Validation')  \n    plt.title('Loss')\n    plt.legend(lines, labels, loc=(1, 0), prop=dict(size=14))\n    plt.show()\n\n  return train_loss_history","metadata":{"tags":[],"cell_id":"35b72d6e673b46d9b43bea464f20c86a","source_hash":"79f5ed44","execution_start":1669369931496,"execution_millis":550,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":" # Re-initialize weights\n#reset_parameters(model)\nmodel = Net() \n\n# Train with defaul settings.\ntrain_loss_history = fit(model, bs=40, epochs=20, workers=0)","metadata":{"tags":[],"cell_id":"e723d1abe048472ea80265bdb44d80a0","source_hash":"14d2e592","execution_start":1669369932046,"execution_millis":96926,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epochs: 20 Batches per epoch: 12 Total number of batches 240\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Net() \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train with defaul settings.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [15], line 74\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, opt_func, lr_scheduler, bs, epochs, batches_per_epoch, show_summary, workers)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     73\u001b[0m   model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Train mode\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     opt\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr_scheduler(t,T,lr\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Forward prop\u001b[39;00m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn [12], line 30\u001b[0m, in \u001b[0;36mMultilabelDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(img_path)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m pre_time \u001b[38;5;241m=\u001b[39m process_time()\n\u001b[0;32m---> 30\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m image \u001b[38;5;241m=\u001b[39m image [:\u001b[38;5;241m3\u001b[39m,:,:] \u001b[38;5;66;03m#remove alpha channel\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loaded_time \u001b[38;5;241m=\u001b[39m process_time()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torchvision/io/image.py:252\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m    251\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torchvision/io/image.py:229\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    228\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":16},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=523d2edd-b418-4a9e-989b-f9b1860f5009' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"2569b25dc7904ddf9bcdb5798f2407eb","deepnote_persisted_session":{"createdAt":"2022-11-09T15:15:44.543Z"},"deepnote_execution_queue":[]}}